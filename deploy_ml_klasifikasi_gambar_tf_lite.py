# -*- coding: utf-8 -*-
"""Deploy ML klasifikasi gambar TF lite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15CylcV004G0XSBgbhQKao3HUyY8H55li
"""

import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
import os
print(tf.__version__)

!wget --no-check-certificate \
http://download850.mediafire.com/n9tcnz08vchg/cawgz6710b93xmw/dataset.zip \
  -O /tmp/dataset.zip

# ekstrasi file zip datasets
import zipfile
local_zip = '/tmp/dataset.zip'
zip_ref = zipfile.ZipFile(local_zip,'r')
zip_ref.extractall('/tmp')
zip_ref.close()

os.listdir('/tmp/dataset')

print("dataset untuk Anjing =",(len(os.listdir('/tmp/dataset/anjing'))))
print("dataset untuk Kucing =",(len(os.listdir('/tmp/dataset/kucing'))))
print("dataset untuk Manusia =",(len(os.listdir('/tmp/dataset/manusia'))))

# membuat nama direktori dan membuat direktori baru 'train' dan 'val'
base_dir = '/tmp/dataset'
train_dir = os.path.join(base_dir,'train')
validation_dir = os.path.join(base_dir, 'val')

os.mkdir(train_dir)
os.mkdir(validation_dir)

anjing_dir = os.path.join(base_dir,'anjing')
kucing_dir = os.path.join(base_dir,'kucing')
manusia_dir = os.path.join(base_dir,'manusia')

# memecah direktori menjadi data train dan data validation
train_anjing_dir, val_anjing_dir = train_test_split(os.listdir(anjing_dir), test_size = 0.2)
train_kucing_dir, val_kucing_dir = train_test_split(os.listdir(kucing_dir), test_size = 0.2)
train_manusia_dir, val_manusia_dir = train_test_split(os.listdir(manusia_dir), test_size = 0.2)

train_anjing = os.path.join(train_dir, 'anjing')
train_kucing = os.path.join(train_dir, 'kucing')
train_manusia = os.path.join(train_dir, 'manusia')

val_anjing = os.path.join(validation_dir, 'anjing')
val_kucing = os.path.join(validation_dir, 'kucing')
val_manusia = os.path.join(validation_dir, 'manusia')

if not os.path.exists(train_anjing):
  os.mkdir(train_anjing)
if not os.path.exists(train_kucing):
  os.mkdir(train_kucing)
if not os.path.exists(train_manusia):
  os.mkdir(train_manusia)

if not os.path.exists(val_anjing):
  os.mkdir(val_anjing)
if not os.path.exists(val_kucing):
  os.mkdir(val_kucing)
if not os.path.exists(val_manusia):
  os.mkdir(val_manusia)

import shutil

for i in train_anjing_dir:
  shutil.copy(os.path.join(anjing_dir, i), os.path.join(train_anjing, i))
for i in train_kucing_dir:
  shutil.copy(os.path.join(kucing_dir,i), os.path.join(train_kucing,i))
for i in train_manusia_dir:
  shutil.copy(os.path.join(manusia_dir,i), os.path.join(train_manusia,i))

for i in val_anjing_dir:
  shutil.copy(os.path.join(anjing_dir, i), os.path.join(val_anjing, i))
for i in val_kucing_dir:
  shutil.copy(os.path.join(kucing_dir,i), os.path.join(val_kucing,i))
for i in val_manusia_dir:
  shutil.copy(os.path.join(manusia_dir,i), os.path.join(val_manusia,i))

print ("train Anjing =",(len(os.listdir('/tmp/dataset/train/anjing'))))
print ("train Kucing =",(len(os.listdir('/tmp/dataset/train/kucing'))))
print ("train Manusia =",(len(os.listdir('/tmp/dataset/train/manusia'))))

print ("val Anjing =",(len(os.listdir('/tmp/dataset/val/anjing'))))
print ("val Kucing =",(len(os.listdir('/tmp/dataset/val/kucing'))))
print ("val Manusia =",(len(os.listdir('/tmp/dataset/val/manusia'))))

# membuat image generator

train_datagen = ImageDataGenerator(
    rescale = 1./225,
    rotation_range = 20,
    zoom_range  = 0.2,
    horizontal_flip = True,
    shear_range = 0.2,
    fill_mode = 'nearest')

test_datagen = ImageDataGenerator(
    rescale = 1./225,
    rotation_range = 20,
    zoom_range  = 0.2,
    horizontal_flip = True,
    shear_range = 0.2,
    fill_mode = 'nearest')

img_shape = (224, 224, 3)
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size= img_shape[:2],
    batch_size = 100,
    class_mode = 'categorical',
    shuffle = True,
)

validation_generator = test_datagen.flow_from_directory(
    validation_dir,
    target_size= img_shape[:2],
    batch_size = 100,
    class_mode = 'categorical',
    shuffle = True,
)

base_model = tf.keras.applications.ResNet50V2(input_shape=img_shape, include_top=False, input_tensor=None)

from keras.constraints import max_norm
model = tf.keras.models.Sequential([
    base_model,
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D((2,2), padding = 'valid'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu', kernel_constraint=max_norm(3)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.summary()

ACCURACY_THRESHOLD = 0.97

class myCallback(tf.keras.callbacks.Callback): 
    def on_epoch_end(self, epoch, logs={}): 
      if(logs.get('accuracy') >= ACCURACY_THRESHOLD):   
        print("\nVal Akurasi Telah mencapai %2.2f%% " %(ACCURACY_THRESHOLD*100))   
        self.model.stop_training = True

callbacks = myCallback()

model.compile(loss = 'categorical_crossentropy',
              optimizer = tf.optimizers.Adam(),
              metrics=['accuracy'])

history = model.fit(
    train_generator,
    steps_per_epoch=100,
    epochs = 100,
    validation_data = validation_generator,
    validation_steps = 10,
    verbose =2,
    callbacks=[callbacks]
)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from google.colab import files
from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline
 
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('akurasi model')
plt.ylabel('akurasi')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='lower right')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('loss model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper right')
plt.show()

print(train_generator.class_indices)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from google.colab import files
from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

uploaded = files.upload()

for fn in uploaded.keys():

#Prediksi Gambar
  path = fn 
  img = image.load_img(path, target_size =(224,224))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images, batch_size=10)

  print(fn)
  if classes[0,0]!=0:
    print('\nPeliharaan Manusia, Anjing')
  elif classes[0,1]!=0:
    print('\nPeliharaan Manusia, Kucing')
  elif classes [0,2]!=0:
    print('\nManusia')

uploaded = files.upload()

for fn in uploaded.keys():

#Prediksi Gambar
  path = fn 
  img = image.load_img(path, target_size =(224,224))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images, batch_size=10)

  print(fn)
  if classes[0,0]!=0:
    print('\nPeliharaan Manusia, Anjing')
  elif classes[0,1]!=0:
    print('\nPeliharaan Manusia, Kucing')
  elif classes [0,2]!=0:
    print('\nManusia')

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)